"""
Automatically generated by Colaboratory.
"""
import numpy as np
import pandas as pd
from keras.datasets import mnist
from keras import models
from keras import layers
from keras.utils import to_categorical
from numpy.random import seed
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, confusion_matrix
import statistics

seed(128) #prevent randomness

#Read in data from csv
data = pd.read_csv("output.csv") #filename
#Pull out labels and data
labels = data['koi_disposition'].to_numpy()
data = data.drop('koi_disposition', axis=1).to_numpy()



#generate kfolds
kf = KFold(n_splits=5, random_state=0, shuffle = True)

#generate empty lists to compute averages later
acc = []
macro = []
micro = []
weighted = []
TP = []
FP = []
FN = []
TN = []
iter = 1

#for each kfold
for train_index, test_index in kf.split(data, labels):
    print("\nK Fold #" + str(iter) + ":")
    X_train, X_test = data[train_index], data[test_index]
    y_train, y_test = labels[train_index], labels[test_index]
    internalacc = []
    internalmacro = []
    internalmicro = []
    internalweighted = []
    internalTP = []
    internalFP = []
    internalFN = []
    internalTN = []
    
    #10 run average for each kfold
    for i in range(10):
        print(i+1, end=" ")
        #neural network in this case performs better with validation data, so split 70-30 train-validation
        #note: "performs betters" in this case means it's easier to tune hyperparameters, it doesn't have large effect on the result.
        split = np.random.rand(len(X_train)) > 0.7
        X_validation = X_train[split]
        Y_validation = y_train[split]
        X_training = X_train[~split]
        Y_training = y_train[~split]
        #store variants of data
        Y_training = to_categorical(Y_training, num_classes = 2)
        Y_validation = to_categorical(Y_validation, num_classes = 2)
        Y_test_nononehot = y_test
        Y_test = to_categorical(y_test, num_classes = 2)
        #generate model
        model = models.Sequential()

        model.add(layers.Dense(512, activation='relu'))
        model.add(layers.Dropout(0.2))
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dropout(0.2))
        model.add(layers.Dense(128, activation='relu'))
        model.add(layers.Dropout(0.2))
        model.add(layers.Dense(2, activation = 'softmax'))

        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

        epochs = 20
        batch_size = 256
        #fit model
        history = model.fit(X_training, Y_training, epochs = epochs, batch_size=batch_size, validation_data=(X_validation, Y_validation), verbose = 0)    

        test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)
        y_prob = model.predict(X_test) 
        y_predclasses = y_prob.argmax(axis=-1)
        y_classes = Y_test.argmax(axis=-1)
        cm=confusion_matrix(y_classes,y_predclasses)
        internalacc.append(test_acc)
        internalmacro.append(f1_score(y_classes, y_predclasses , average="macro"))
        internalmicro.append(f1_score(y_classes, y_predclasses , average="micro"))
        internalweighted.append(f1_score(y_classes, y_predclasses , average="weighted"))
        internalTP.append(cm[0,0])
        internalFP.append(cm[0,1])
        internalFN.append(cm[1,0])
        internalTN.append(cm[1,1])
    #calc statistics
    acc.append(statistics.mean(internalacc))
    macro.append(statistics.mean(internalmacro))
    micro.append(statistics.mean(internalmicro))
    weighted.append(statistics.mean(internalweighted))
    TP.append(statistics.mean(internalTP))
    FP.append(statistics.mean(internalFP))
    FN.append(statistics.mean(internalFN))
    TN.append(statistics.mean(internalTN))
    iter += 1
print("\n\nRESULTS:")
print("Acc:", statistics.mean(acc))
print("F1 Macro:", statistics.mean(macro))
print("F1 Micro:", statistics.mean(micro))
print("F1 Weighted:", statistics.mean(weighted))
print("TP:", statistics.mean(TP))
print("FP:", statistics.mean(FP))
print("FN:", statistics.mean(FN))
print("TN:", statistics.mean(TN))

